# A Review of My Last 10 Years of Study and Research

Date: 2025.09.15 | Author: Zihan Ding

It all started 10 years ago. After my college entrance exam, I faced the choice of a major for my undergraduate studies. The question I asked myself was which subject would maximize my impact if I studied it—what would have the maximal impact on the world. For me, the answer was artificial intelligence. I’m not sure how I knew this term back in 2014—it was even before AlphaGo came out. Maybe it was from a newspaper (I read the newspaper every day at night before the entrance exam; it was indeed a weird hobby). My reasoning was simple: if we had a human-level intelligent agent, it could replace human study or any brain labor, and it should, in the end, maximize impact if it worked—what later turned out to be the so-called "Singularity".

So I chose two majors (the final decision was made by the university): physics and computer science. I chose physics because I had spent time on the physics olympiad in China during high school, and it was truly an attractive subject. In the end, I entered the physics department at University of Science and Technology of China (USTC). USTC has one of the best physics departments in China, and I thought there would be nothing wrong with “spending” four years exploring a subject I found deeply appealing, full of accumulated human wisdom, so I was okay with this result. But I wasn’t fully satisfied, because I knew my final path should be AI or computer science, so in my second undergraduate year I chose CS as my second major. It was really painful to take two majors at the same time—the three CS courses each semester took the entire Saturdays from 9 a.m. to 7 p.m. I still remember my full calendar from Monday to Sunday and 10 main courses per semester. I thought there was nothing bad about learning more, but it turned out to be a bad strategy for PhD applications: it exhausted me and hurt my GPA. In the end, I completed about 240 credits, while the normal requirement for a single major was only 160.

## Early Research Interests

During undergrad, I had two main focuses in research and extra-curricular study: robotics and physics. From my first year, I joined a robotics competition, which lasted about two years. I felt robotics would be quite important in the future, but there was still a long, long way to go. I also thought it would be cool if maching learning methods could be used to control robots, but I didn’t know how at that time. In each competition, we used only traditional control methods, mostly PID-based feedback control. I still remember the old days: we took a 22-hour bus (to carry all the robots) to Xiamen for a competition, and the whole team debugged, assembled, and disassembled robots overnight in an underground garage at 35 degrees, shirts off. The competition was hosted by DJI, now the world’s largest UAV company. Those experiences planted a seed in my heart that later grew into my decision to work heavily on reinforcement learning and robotics during my master’s degree.

My other focus was physics. Even within physics, my final degree was more fine-grained, in quantum computation. I spent my last two undergraduate years in a trapped-ion lab, learning and experimenting with simple quantum systems. I still remember how excited I was when I saw a single trapped Yb ion centered in the camera view in that dark room (no light is allowed in this type of lab, and it’s usually underground). I appreciate my mentor, Jinming Cui, for walking me through the basics of NumPy in Jupyter Notebook to control the quantum system, since I was a physics major working mostly on theory and concepts, without much Python programming background. The work I finally completed was a machine-learning-assisted method for qubit tomography with some FPGA hardware acceleration—and yes, my first publication was actually a physics paper. Put simply, it was the readout of the quantum state of a bit. Although I enjoyed the process, I could feel that if we struggled so much even with single or several qubits—not to mention basic logic gates—then quantum computation had a very long way to go. Therefore, I dropped this direction and fully switched to machine learning in my master’s.

## First Encounters with Reinforcement Learning

My first reinforcement learning project was in my third undergraduate year, when I worked on a NIPS (yes—before NeurIPS, that was its name) competition to make a simulated human skeleton run. That was the moment I saw the power of RL and it strengthened my choice to pursue this direction. I didn’t see any other elegant way to solve this problem—or any way at all. To this day, I still think RL is the best method to solve such high-dimensional control problems with a simple numerical goal in simulation. We used DDPG and spent months tuning performance. Since it was a competition, there wasn’t much “research,” but there was a very high density of engineering optimization. With a bunch of tricks, we placed 4th out of more than 400 teams worldwide. The first place was a company team founded by Jurge Schimidburg; they used PPO with a very wide network. PPO came out in the middle of the competition, so most teams were still using DDPG. One critical improvement in performance came from scaling (what an analogue to LLMs). At the beginning, with a single learning process we barely saw improvement in the learning curve because the problem was so high-dimensional. Multiprocess parallel sampling—proposed by a hero in the competition community—suddenly improved the average performance of all top teams. I even cold-emailed John Schulman (who later co-founded OpenAI and now Thinking Machine Lab) to ask about PPO’s value function update when a batch contains more than one episode, and he kindly replied. Around that time, I also connected with Yuhuai Wu (now co-founder of xAI) when he was working on KFAC. People grow incredibly fast in such a high-pace domain.

## Master’s Studies at Imperial College

At Imperial College, I took world-class courses covering the foundations of machine learning. Michael Bronstein
 (OG of Graph Neural Networks) taught us deep learning; Marc Deisenroth (OG of variational inference and reinforcement learning) taught probabilistic inference; and Andrew Davison (OG of SLAM) taught robotics. I also learned Transformers in an NLP course but barely paid attention—what a pity. Based on my previous experiences, reinforcement learning for robotic control became my natural research direction, and I joined Edward Johns’s Robot Learning Lab. Ed is very patient in guiding students in research, and I was glad to work closely with him for more than a year.

After a lot of RL work and tuning on robot manipulation, I considered starting a company after my master’s graduation (2019) to build a learning-based general-purpose robot. But I felt RL with sim-to-real techniques was not the right paradigm, due to the highly dexterous nature of manipulation, the difficulty of designing reward functions for each specific task, the heavy engineering needed to co-tune simulation and real-world policies, and the lack of generalization beyond training tasks. So I gave up that idea and decided to pursue a PhD. Perhaps because most of my robotics research had been coding-based and application-driven, I felt my theoretical understanding of RL was lacking, so I contacted my future PhD advisor, Chi Jin, to gain a deeper understanding about RL theory and algorithm design. When I applied for PhDs, I also asked Peter Dayan for advice; he suggested I “*focus on your true interests instead of what other people want you to do or what is popular*”.

## The Deep RL Book

Apart from robotics research, I worked on open-sourcing RL implementations during my master’s. Collaborating with several others, we built an RL algorithm tutorial for TensorLayer (a TensorFlow ML modules repo, a competitor to Keras) and a standalone RL repo. Probably because of this, a publisher—Packt Publishing—reached out to collaborate on a new RL book. After discussion, we had divergent opinions on the book’s scope, so we declined. However, we still felt there was a need to summarize RL advances from 2015 to 2019, so we decided to write the book ourselves. It became a two-year project: the English version took one year, and the Chinese translation took another (ChatGPT would have helped a lot, but it didn’t exist back then!). I spent most of my spare time composing and proofreading chapters throughout those two years. Our team was highly responsible, and I enjoyed the collaboration. At IJCAI 2019 in Macau, I met a Springer editor. I told her we were two-thirds done on a new RL book, and she kindly agreed to collaborate on publishing later.

## Industry Internships

After my master’s, I contacted Matthew Taylor for further RL research. He introduced me to Borealis AI for my first research internship at a company, so I went to Toronto for five months. The company focused on AI research with a finance background, and they found that financial technicians tended not to trust ML model predictions. Therefore, I worked on project of explainable RL policies to improve interpretability of agents’ decisions. However, after reading through the book *Interpretable Machine Learning* by Christoph Molnar and other methods, I felt this problem might be ill-posed—most methods provide ad hoc, posterior explanations rather than true interpretation. Even worse, I was trapped at home for four months in Toronto, where I almost knew no one, due to the 2020 pandemic.

After that, I returned to China and joined Tencent Robotics X for more robot manipulation research. Again, due to the pandemic and visa policy, I deferred my PhD admission for a year, even though I had already been admitted. This work followed naturally from my research at Imperial College on RL and sim-to-real for manipulation, focusing on efficient sim-to-real methods and tactile-assisted robot learning. Some of my X Lab colleagues started or joined robotics startups in China in 2024–2025, following the embodied AI trend.

## Study at Princeton

In the first two years of my PhD, I took most ML theory courses to strengthen my foundation: Chi Jin’s optimization, machine learning theory, and RL theory; Sanjeev Arora’s deep learning theory; and Elad Hazan’s online learning theory. Fortunately, I also took Danqi Chen’s NLP course in Fall 2022, right before the release of ChatGPT. In my spare time, I wondered about the mysteries of human consciousness. In November 2021, three friends and I started a weekend reading group on consciousness-related papers that lasted almost two years and culminated in a survey on consciousness from a computational perspective, with some propositions for conscious AI systems. In March 2025 at Princeton, I gladly attended an offline talk by David Chalmers and Michael Graziano and enjoyed their debate on the necessary components of subjective experience and its relationship with self-awareness. This project fulfilled my underlying curiosity about unknown problems, aligning with Peter Dayan’s suggestion. In my other spare time, I took courses on Web3 and blockchain taught by Pramod Viswanath, Matt Weinberg, and Jaswinder Pal Singh, as an extension of my interest in game theory applications and to satisfy my curiosity. What impressed me most was the study of incentive mechanism design in decentralized autonomous systems, which I believe will play a significant role in a future society populated by fully autonomous AI agents.


## Research During the PhD

My first Princeton project was finding Nash equilibri (NE) in two-player zero-sum video games, with a new algorithm verified in deep learning settings. It was a hard project; I spent more than a year on it and made over 2,300 commits to that GitHub repo. The difficulty lay in building a unified framework to fairly compare several heterogeneous multi-agent RL algorithms across environments, requiring asynchronous and synchronous sampling and training for efficiency. Also, the OpenAI Gym repo was transitioning to Gymnasium at that time, with incompatible dependencies and changing environment arguments. We finally achieved decent results with close-to-NE policies that were hard to exploit by a standard DQN opponent, but the project did not attract much attention from the research community. After exploring multi-agent RL in other settings, I shifted my focus to diffusion models and foundation models for the second half of my PhD, although still trust in the RL paradigm. I am grateful to my advisor, Chi Jin, for providing me with the flexibility and support to explore new directions.

One main stream of this research was leveraging diffusion models’ expressive power for RL agents—as both policy and environment modeling. My initial work at Meta FAIR Lab on diffusion-based world modeling used a latent state space to reduce computational burden and focus on RL algorithm design, but I realized diffusion’s strength lies more in higher-dimensional spaces. However, for interactive world modeling, this is limited by sampling efficiency. So in the following year’s project at Adobe, I worked on efficiency improvements for large-scale video generation models. There are still several missing pieces for useful foundational world modeling, and it is a multimodal problem. With Internet text data increasingly exhausted, we need to move toward mixed-modality training to absorb more information into models and effectively align latent representations across modalities in a unified framework. Unlike text—which is a summary with strong human priors about the world—the visual modality is freer of prior assumptions and only constrained by the principles of the universe, and it is effectively unlimited in amount because it does not rely on human generation processes.

## Reflections on AGI and ASI

In terms of paradigm, standard RL is strong at improving any constructed metric with verifiable answers but is limited by human capacity to provide judgments first. I think this paradigm can achieve AGI over time, with my definition of artificial general intelligence (AGI) as “*a system that can beat most average humans at most (or all) common intellectual tasks*”.

If common tasks include the physical tasks of daily human life, then an AGI system should naturally extend to supporting general-purpose robots. However, given the scarcity of physical-world data, I doubt such robots will emerge anytime soon. For this reason, the definition of AGI here emphasizes “intellectual” tasks, deliberately excluding physical ones. From my perspective, building a general-purpose robot may in fact require the prior existence of AGI in intellectual tasks, at least for visual understanding, semantic reasoning and planning. Only then could we claim to have a true AGI system—one that surpasses the average human across all common tasks. 

Beyond the current human limit, automating verification and further optimizing the system with RL can surpass human intelligence through mutual bootstrapping across models. This may achieve artificial superintelligence (ASI), which I define as “*a system that can beat the best human in each common intellectual task*”.

As for the consequences of AGI and ASI, I am concerned about the potential polarization of social power and wealth. A small minority that controls such superintelligence could come to dominate the economy and even governing authority. At present, I don’t see a clear solution to this problem.


**Salute to all the people I met along the way who supported and helped me.**

This blog summarizes the gap between theoretical and empirical research in reinforcement learning (RL), especially deep RL, as the case when neural networks are used for function approximation.



* Most theoretical analysis only cares statistical upper bounds and lower bounds, while most practical cases fall in between, and the theory does not capture the best methods for those middle fields. Considering the autonomous driving as an example, it's generally not hard to create an adversarial example outside the realistic distribution to attack a well-trained model, like creating an artificial image and put it in front of the camera of the vehicle to intentionally confuse the model to make wrong predictions. This would directly lead to a failure case of the model prediction\ref{}. If theoretical upper bound of this type of attack is considered in the model training, no matter which method is used, the problem of learning a theoretically unattackable model is going to be super hard and less feasible. A very loose bound will be derived in this case. However, as a practical consideration, there would be very rare to see such a thing happening in practice, a majority of the cases the vehicle will encounter fall in the region that is strongly regularized by the fact of reality, and a model efficiently and effectively working in this domain is generally acceptable, or even used day in and day out by humans. The underlying algorithm of human mind is not an algorithm to guarantee the best performance under the worse case, but the very efficient one to work relatively well in the dominated cases in terms of the probability.  

  optimistic is an example, think about why optimistic makes no sense if the goal is just to optimize the expected return from stock trading, optimistic will lead to potentially better results to match the upper bound for the worst case example, but it may also lead to worse results for most middle case examples

* Current theory does not capture the sample efficiency for algorithms with function approximations like neural networks, i.e., the deep RL cases. Most of the theoretical analysis of RL algorithms still lie in the regime of tabular settings, where both the state space $\mathcal{S}$, action space $\mathcal{A}$ are discrete, finite and small. For example, the information theoretical lower bound for solving optimal policy in single-agent tabular MDP is $\Omega(\frac{H^3SA}{\epsilon^2})$. $Q$-learning~\ref{} algorithm with upper confidence bound (UCB) for exploration (not $\epsilon$-greedy) is provably convergent to optimal with $\tilde{\mathcal{O}}(\frac{H^4SA}{\epsilon^2})$ rate. From a statistical viewpoint, there is only a $H$ factor larger than the lower bound, which is almost perfect. However, if directly plugging in these results in to a non-tabular case, e.g., continuous state or action space, $S$ or $A$ directly goes to infinite. Discretization or abstraction techniques can be applied to alleviate the problem, but still far from an efficient scope. A small fraction of the works analyze the RL algorithms beyond the tabular case, including linear or kernel function class, low Bellman-rank cases, occasionally general function approximation. However, when it comes to the neural networks, the convergence upper bound of the methods become too loose to provide some empirical insights.

* Most algorithm complexity is described with sample efficiency but not computational efficiency, where a huge difference is in between. 

* Theoretical analysis usually does not care about the implementation difficulty of an algorithm, but it really matters for the empirical side; no body will take a trick that is much harder to implement in practice but has a slightly better rate in the worst case (say a lower exponential dependency ).

* theory may improve a dependency on a certain variable, but it really doesn't matter in practice, for example, the variable itself can be of infinite value in practice, or super small compared with other dependent variables.

* There are usually tons of engineering tricks in empirical algorithms, with each one of them potentially overshadowing the effectiveness the deliberate design of a theoretical technique in the algorithm. For example,  

* Practically impossible formulation when using function approximation. For example, a policy that is an averaging over a set of policies is feasible only for the tabular case, where the policy can be explicitly written down or recorded in a table. However, when the function approximation is applied to represent a policy, it hard or impossible to directly take an average over a set of neural networks, each as an function approximator of a specific policy. Approximation of the averaging operation is required in previous work. So when this kind of practically impossible formulation appears in theoretical work, the error of approximation as a necessary component in empirical implementation has to be additionally considered. 

* Prior knowledge is usually overlooked. 
